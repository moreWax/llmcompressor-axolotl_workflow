# llmcompressor-axolotl_workflow
Train openai/gpt-oss-120b (117 B MoE) with 16 k-token context on consumer GPUs by combining 4-bit quantisation, 2:4 sparsity and Axolotlâ€™s sequence parallelism.
